Metadata-Version: 2.4
Name: qntx-webscraper
Version: 0.1.0
Summary: QNTX Python plugin for web scraping and URL attestation
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: grpcio>=1.60.0
Requires-Dist: grpcio-tools>=1.60.0
Requires-Dist: protobuf>=4.25.0
Requires-Dist: beautifulsoup4>=4.12.0
Requires-Dist: requests>=2.31.0
Requires-Dist: lxml>=5.0.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Requires-Dist: ruff>=0.1.0; extra == "dev"

# QNTX Webscraper Plugin

A Python plugin for QNTX that scrapes web pages, extracts URLs, and creates attestations.

## Overview

This plugin demonstrates how to write QNTX domain plugins in Python using the gRPC external plugin interface. It:

- Scrapes web pages and extracts all links
- Creates attestations for page relationships (`links_to`, `links_externally_to`)
- Supports crawling multiple pages from a starting URL
- Communicates with QNTX via gRPC

## Installation

Using `uv` (recommended):

```bash
cd qntx-webscraper

# Create virtual environment and install dependencies
uv venv
uv pip install -e .

# Generate gRPC stubs from QNTX proto files
uv pip install grpcio-tools
./generate_grpc.sh
```

## Running the Plugin

Start the plugin as a gRPC server:

```bash
uv run qntx-webscraper --port 9001
```

Or with more options:

```bash
uv run qntx-webscraper --port 9001 --log-level DEBUG
```

## Configuration

Configure in QNTX's `am.toml`:

```toml
[plugins.webscraper]
enabled = true
external = true
endpoint = "localhost:9001"

[plugins.webscraper.config]
user_agent = "QNTX-WebScraper/0.1"
timeout = "30"
```

## HTTP Endpoints

Once connected to QNTX, the plugin exposes these endpoints under `/api/webscraper/`:

### POST /scrape

Scrape a URL and return extracted links (no attestations created).

```json
{
  "url": "https://example.com"
}
```

Response:

```json
{
  "url": "https://example.com",
  "title": "Example Domain",
  "status_code": 200,
  "links": [
    {
      "target_url": "https://www.iana.org/domains/example",
      "anchor_text": "More information...",
      "is_external": true,
      "rel": []
    }
  ]
}
```

### POST /scrape-and-attest

Scrape a URL and create attestations for all found links.

```json
{
  "url": "https://example.com",
  "actor": "webscraper-user",
  "include_external": true
}
```

Response:

```json
{
  "url": "https://example.com",
  "title": "Example Domain",
  "links_count": 5,
  "attestations_created": 6,
  "attestation_ids": ["as_...", "as_..."]
}
```

### POST /crawl

Crawl from a starting URL and create attestations for all pages.

```json
{
  "url": "https://example.com",
  "actor": "webscraper-crawler",
  "max_pages": 10,
  "same_domain_only": true
}
```

## Attestation Schema

The plugin creates attestations with these predicates:

| Predicate | Subject | Context | Description |
|-----------|---------|---------|-------------|
| `has_title` | page URL | title text | Page title |
| `links_to` | page URL | target URL | Internal link |
| `links_externally_to` | page URL | target URL | External link |

Attestation attributes include:
- `source`: "qntx-webscraper"
- `anchor_text`: Link text (if available)
- `rel`: Comma-separated rel attribute values

## Development

```bash
# Install dev dependencies
uv pip install -e ".[dev]"

# Run tests
uv run pytest

# Format code
uv run ruff format .

# Lint
uv run ruff check .
```

## Architecture

```
qntx-webscraper/
├── qntx_webscraper/
│   ├── __init__.py
│   ├── main.py          # Entry point
│   ├── plugin.py        # gRPC DomainPlugin service
│   ├── scraper.py       # Web scraping logic
│   ├── atsstore.py      # ATSStore gRPC client
│   └── grpc/            # Generated protocol stubs
│       ├── atsstore_pb2.py
│       ├── atsstore_pb2_grpc.py
│       ├── domain_pb2.py
│       └── domain_pb2_grpc.py
├── pyproject.toml
├── generate_grpc.sh
└── README.md
```

The plugin runs as a standalone gRPC server. QNTX connects to it and:
1. Calls `Initialize` with the ATSStore endpoint and auth token
2. Proxies HTTP requests to `HandleHTTP`
3. Calls `Shutdown` on termination

## Why Python for Plugins?

Python excels for web scraping due to:
- **BeautifulSoup/lxml**: Robust HTML parsing
- **requests**: Simple HTTP client
- **Scrapy**: Full-featured crawling (can be integrated)
- **Rapid prototyping**: Quick iteration on extraction logic

The gRPC interface means any language can be used for QNTX plugins while maintaining type safety through protobuf.
