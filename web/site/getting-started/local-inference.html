<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>QNTX - local-inference</title>
  <link rel="icon" type="image/jpeg" href="../qntx.jpg">
  <link rel="stylesheet" href="../css/core.css">
  <link rel="stylesheet" href="../css/docs.css">
</head>
<body>
<nav class="doc-nav">
  <a href="../index.html">
    <img src="../qntx.jpg" alt="QNTX" class="site-logo">Documentation Home
  </a>
</nav>
<h1>Local Inference Setup</h1>
<h2>Why Local Inference?</h2>
<p><strong>Privacy, cost, and control.</strong> Cloud LLM APIs are convenient but:</p>
<ul>
<li><strong>Cost</strong>: $0.001-0.01+ per API call adds up fast</li>
<li><strong>Privacy</strong>: Your data goes to third-party servers</li>
<li><strong>Latency</strong>: Network round-trips add 200-1000ms</li>
<li><strong>Availability</strong>: Internet required, rate limits apply</li>
</ul>
<p><strong>Local inference</strong> runs models on your hardware. Zero API cost, complete privacy, works offline.</p>
<h2>Why Ollama?</h2>
<p><strong>Simplest path from zero to working LLM.</strong> No Python, no virtual environments, no CUDA drivers (unless you want GPU acceleration). Download binary, pull model, run.</p>
<p>Alternative (LocalAI) exists but Ollama's UX is unmatched for getting started.</p>
<h2>Quick Start</h2>
<h3>1. Install Ollama</h3>
<pre><code class="language-bash"># macOS / Linux
curl -fsSL https://ollama.com/install.sh | sh

# Or download: https://ollama.com/download
</code></pre>
<h3>2. Download a Model</h3>
<pre><code class="language-bash"># Recommended: Fast, general-purpose (7B params, 4GB)
ollama pull mistral

# Alternative: Smaller, faster (3B params, 2GB)
ollama pull llama3.2:3b

# For code: Optimized for technical content (7B, 4.5GB)
ollama pull qwen2.5-coder:7b
</code></pre>
<p><strong>Why these models?</strong> Balance of size/speed/quality. Smaller models (3B) are fast on CPU. Larger models (7B) give better results but need more RAM.</p>
<h3>3. Start Ollama</h3>
<pre><code class="language-bash">ollama serve
</code></pre>
<p>Runs on <code>http://localhost:11434</code> by default.</p>
<h3>4. Configure QNTX</h3>
<p>Edit <code>~/.qntx/am.toml</code> (or project <code>am.toml</code>):</p>
<pre><code class="language-toml">[local_inference]
enabled = true
base_url = "http://localhost:11434"
model = "mistral"
timeout_seconds = 120
</code></pre>
<p><strong>Done.</strong> All QNTX LLM operations now use local inference.</p>
<h2>Verify Setup</h2>
<pre><code class="language-bash"># Check Ollama is running
curl http://localhost:11434/api/tags

# Test inference
curl http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistral",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
</code></pre>
<h2>Performance: CPU vs GPU</h2>
<p><strong>CPU inference works but is slow</strong> (5-10 tokens/sec). Fine for development, frustrating for production.</p>
<p><strong>GPU acceleration</strong> makes local inference viable:</p>
<ul>
<li><strong>Apple Silicon (M1/M2/M3)</strong>: Auto-detected, 5-20x faster than CPU</li>
<li><strong>NVIDIA GPU</strong>: Requires CUDA, 10-100x faster than CPU</li>
</ul>
<p>Ollama automatically uses GPU if available. No configuration needed.</p>
<h3>Model Size Tradeoffs</h3>
<table><thead><tr><th>Model</th><th>Size</th><th>CPU Speed</th><th>GPU Speed</th><th>Quality</th><th>Best For</th></tr></thead><tbody>
<tr><td>llama3.2:3b</td><td>2GB</td><td>Very Slow</td><td>Fast</td><td>Good</td><td>Testing, quick tasks</td></tr>
<tr><td>mistral</td><td>4GB</td><td>Slow</td><td>Very Fast</td><td>Excellent</td><td>General purpose</td></tr>
<tr><td>qwen2.5-coder:7b</td><td>4.5GB</td><td>Slow</td><td>Very Fast</td><td>Excellent</td><td>Code/technical</td></tr>
</tbody></table>
<p><strong>Why not 13B or 70B models?</strong> Possible but require 8GB+ VRAM and are slower. Diminishing returns for most QNTX operations.</p>
<h2>Cost Tracking</h2>
<p><strong>Local inference has zero API cost</strong> but uses local resources (GPU/CPU time, electricity).</p>
<p>QNTX budget system is API-cost focused. Future versions may track GPU time:</p>
<pre><code class="language-toml">[pulse]
# Current: Only tracks cloud API spend
daily_budget_usd = 5.00

# Future: Track GPU resource usage
# max_gpu_minutes_per_day = 30.0
</code></pre>
<p>See <code>pulse/budget/</code> package TODOs for GPU resource tracking plans.</p>
<h2>Switching Between Local and Cloud</h2>
<p><strong>Why switch?</strong> Local for bulk operations (save money), cloud for occasional high-quality needs (GPT-4, Claude).</p>
<pre><code class="language-bash"># Enable local inference
qntx config set local_inference.enabled true

# Disable (use cloud APIs)
qntx config set local_inference.enabled false
</code></pre>
<p>Configuration reloads automatically. No restart required.</p>
<h2>Troubleshooting</h2>
<h3>Connection Refused</h3>
<p><strong>Cause</strong>: Ollama server not running.</p>
<p><strong>Fix</strong>: <code>ollama serve</code></p>
<h3>Model Not Found</h3>
<p><strong>Cause</strong>: Model not downloaded.</p>
<p><strong>Fix</strong>: <code>ollama pull mistral</code></p>
<h3>Slow on CPU</h3>
<p><strong>Cause</strong>: CPU inference is inherently slow.</p>
<p><strong>Options</strong>:</p>
<ol>
<li>Use smaller model: <code>ollama pull llama3.2:3b</code></li>
<li>Increase timeout: <code>timeout_seconds = 300</code></li>
<li>Get GPU-enabled hardware</li>
<li>Use cloud APIs for now</li>
</ol>
<h3>GPU Not Detected (NVIDIA)</h3>
<p><strong>Check</strong>: <code>nvidia-smi</code></p>
<p><strong>Fix</strong>: Install CUDA toolkit (Ollama will use it automatically)</p>
<p><strong>Apple Silicon</strong>: Works automatically, no setup needed</p>
<h2>Advanced: Custom Models</h2>
<p><strong>Why custom models?</strong> Specialized system prompts, custom parameters, fine-tuned weights.</p>
<p>Create <code>Modelfile</code>:</p>
<pre><code>FROM mistral

SYSTEM You are a code review assistant. Focus on security and correctness.

PARAMETER temperature 0.7
PARAMETER num_predict 2048
</code></pre>
<p>Build and use:</p>
<pre><code class="language-bash">ollama create qntx-reviewer -f Modelfile
</code></pre>
<p>Configure in <code>am.toml</code>:</p>
<pre><code class="language-toml">[local_inference]
model = "qntx-reviewer"
</code></pre>
<h2>When to Use Local vs Cloud</h2>
<p><strong>Use local if:</strong></p>
<ul>
<li>You have GPU (Apple Silicon or NVIDIA)</li>
<li>Bulk operations (cost savings matter)</li>
<li>Privacy/security requirements</li>
<li>Offline usage needed</li>
</ul>
<p><strong>Use cloud if:</strong></p>
<ul>
<li>No GPU and need speed</li>
<li>Occasional/low-volume usage</li>
<li>Want frontier models (GPT-4, Claude)</li>
<li>Minimal setup time</li>
</ul>
<h2>Alternative: LocalAI</h2>
<p><strong>Why consider LocalAI?</strong> Written in Go (like QNTX), supports many model formats, self-contained binary.</p>
<p><strong>Why Ollama instead?</strong> Better UX, faster iteration, larger community, simpler setup.</p>
<p>LocalAI is viable if you need specific model formats Ollama doesn't support.</p>
<h3>Quick Start (Docker)</h3>
<pre><code class="language-bash">docker run -p 8080:8080 localai/localai:latest
</code></pre>
<p>Configure:</p>
<pre><code class="language-toml">[local_inference]
enabled = true
base_url = "http://localhost:8080"
model = "your-model-name"
</code></pre>
<h2>Resources</h2>
<ul>
<li><strong>Ollama</strong>: https://ollama.com</li>
<li><strong>Model Library</strong>: https://ollama.com/library</li>
<li><strong>LocalAI</strong>: https://localai.io</li>
<li><strong>QNTX GPU Resource Plans</strong>: <code>pulse/budget/</code> package TODOs</li>
</ul>
<footer class="site-footer">
  <p class="provenance">Generated by sitegen.nix at commit <a href="https://github.com/teranos/QNTX/commit/0e7ca38dd1dcce5d67e9797b91950b21353f137a-dirty">0e7ca38-dirty</a></p>
</footer>
</body>
</html>
