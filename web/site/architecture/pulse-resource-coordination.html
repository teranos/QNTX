<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>QNTX - pulse-resource-coordination</title>
  <link rel="icon" type="image/jpeg" href="../qntx.jpg">
  <link rel="stylesheet" href="../css/core.css">
  <link rel="stylesheet" href="../css/docs.css">
</head>
<body>
<nav class="doc-nav">
  <a href="../index.html">
    <img src="../qntx.jpg" alt="QNTX" class="site-logo">Documentation Home
  </a>
</nav>
<h1>Pulse Multi-Process Resource Coordination</h1>
<p><strong>Status:</strong> Design Proposal (Issue #50)
<strong>Related:</strong> pulse/budget package, pulse/async worker pool</p>
<h2>Problem</h2>
<p>Running QNTX on shared infrastructure (beefy server with GPU) alongside other processes:</p>
<ul>
<li>Training jobs, inference services, data processing</li>
<li>Each process has resource quotas (e.g., "max 30% GPU capacity")</li>
<li>Current limitation: QNTX only tracks internal usage, doesn't coordinate with other processes</li>
<li>Need to ensure QNTX respects system-wide load and plays nice with competing processes</li>
</ul>
<h2>Solution: Multi-Level Resource Coordination</h2>
<h3>1. System-Wide Resource Monitoring</h3>
<p>Track actual resource utilization across ALL processes, not just QNTX:</p>
<p><strong>Resources to monitor:</strong></p>
<ul>
<li>GPU: nvidia-smi for utilization, memory usage, running processes</li>
<li>CPU: /proc/stat or system APIs</li>
<li>Memory: System memory pressure (not just QNTX allocations)</li>
<li>Disk I/O: Throughput monitoring for heavy data loading</li>
</ul>
<p><strong>Implementation sketch:</strong></p>
<pre><code class="language-go">type SystemResourceMonitor struct {
    GPUUtilizationPercent  float64  // Current GPU load (0-100%)
    GPUMemoryUsedMB        int      // Total VRAM in use by all processes
    SystemCPUPercent       float64  // Overall CPU usage
    SystemMemoryUsedMB     int      // Total RAM in use
    QNTXProcessShare       float64  // qntx's estimated share (0-1)
}

func (m *SystemResourceMonitor) GetCurrentLoad() (*SystemResourceMonitor, error) {
    // Call nvidia-smi, parse output
    // Read /proc/stat for CPU
    // Read /proc/meminfo for memory
}
</code></pre>
<h3>2. Dynamic Quota Adjustment Based on System Load</h3>
<p>Adapt QNTX behavior to current system contention:</p>
<ul>
<li><strong>Low load</strong> (GPU &lt;30%): Use full allocated quota</li>
<li><strong>Medium load</strong> (GPU 30-70%): Reduce QNTX quota to 50%</li>
<li><strong>High load</strong> (GPU &gt;70%): Throttle to minimum (10% or pause)</li>
</ul>
<pre><code class="language-go">func (bt *Tracker) GetAdaptiveQuota() float64 {
    sysLoad, _ := bt.sysMonitor.GetCurrentLoad()
    baseQuota := bt.config.DailyGPUMinutes  // e.g., 30 GPU-minutes/day

    // Apply backpressure based on system contention
    if sysLoad.GPUUtilizationPercent &gt; 70 {
        return baseQuota * 0.1  // Throttle to 10% when system busy
    } else if sysLoad.GPUUtilizationPercent &gt; 30 {
        return baseQuota * 0.5  // Reduce to 50% during medium load
    }
    return baseQuota  // Full quota when system idle
}
</code></pre>
<h3>3. Backpressure Mechanisms</h3>
<p>Pause or slow down job processing when other processes need resources:</p>
<ul>
<li>Check system load before each job dequeue</li>
<li>If system busy, delay execution with exponential backoff</li>
<li>Emit logs: "System under load, deferring job for 30s"</li>
</ul>
<p><strong>Integration point:</strong> pulse/async/worker.go processJobs() loop</p>
<pre><code class="language-go">func (w *Worker) processJobs(ctx context.Context) {
    for {
        // Check system load before dequeuing
        if systemLoad := w.sysMonitor.GetCurrentLoad(); systemLoad.GPUUtilizationPercent &gt; 80 {
            w.logger.Info("System under load, deferring job processing",
                "gpu_util", systemLoad.GPUUtilizationPercent)
            time.Sleep(30 * time.Second)
            continue
        }
        // ... proceed with job dequeue
    }
}
</code></pre>
<h3>4. Cooperative Scheduling with Other Processes</h3>
<p>Use OS-level mechanisms to coordinate:</p>
<ul>
<li><strong>cgroups (Linux)</strong>: Respect CPU/memory limits set by container runtime</li>
<li><strong>Process nice values</strong>: Lower QNTX priority when system busy</li>
<li><strong>File-based locking</strong>: Coordinate GPU access via /tmp/gpu.lock (primitive but works)</li>
<li><strong>Advanced</strong>: Shared memory or IPC for coordination with other QNTX instances</li>
</ul>
<h3>5. Container Orchestration Integration</h3>
<p>Respect resource limits set by K8s/Docker:</p>
<ul>
<li>Read cgroup limits: <code>/sys/fs/cgroup/memory/memory.limit_in_bytes</code></li>
<li>Honor K8s resource requests/limits from Pod spec</li>
<li>Use Kubernetes Downward API for resource allocation</li>
</ul>
<p><strong>Example:</strong> If K8s sets <code>limits.memory = 8Gi</code>, <code>limits.nvidia.com/gpu = 1</code>, QNTX should never exceed those limits even if internal config says otherwise.</p>
<h3>6. Graceful Degradation Under Contention</h3>
<p>Prioritize critical jobs when resources are scarce:</p>
<ul>
<li><strong>High priority</strong>: User-initiated operations (blocking CLI with --sync)</li>
<li><strong>Medium priority</strong>: Async job processing</li>
<li><strong>Low priority</strong>: Background data ingestion (bulk imports)</li>
</ul>
<p><strong>Implementation:</strong> Add priority field to Job struct, check system load + priority before dequeue.</p>
<h2>Defensive: Detecting Non-Cooperative Processes (GPU Hogging)</h2>
<h3>Problem</h3>
<p>What if another process doesn't play nicely?</p>
<ul>
<li>Training job runs 24/7 at 100% GPU</li>
<li>Inference service doesn't respect quotas, uses all VRAM</li>
<li>Rogue process leaks GPU memory</li>
</ul>
<h3>Detection Strategies</h3>
<p><strong>a) Per-Process GPU Monitoring:</strong></p>
<pre><code class="language-bash">nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv
</code></pre>
<p>Track per-process utilization over time to identify hogs.</p>
<p><strong>b) Sustained High Utilization Detection:</strong></p>
<p>Flag processes sustaining &gt;80% GPU for &gt;10 minutes:</p>
<pre><code class="language-go">type ProcessGPUStats struct {
    PID              int
    Name             string
    GPUUtilPercent   float64
    VRAMUsedMB       int
    DurationMinutes  float64
}

func (m *SystemResourceMonitor) DetectGPUHogs(threshold float64, duration time.Duration) []ProcessGPUStats {
    // Returns processes exceeding threshold for longer than duration
}
</code></pre>
<p><strong>c) Fair Share Violation Detection:</strong></p>
<p>If your quota is 30% GPU and you measure:</p>
<ul>
<li>Total GPU utilization: 95%</li>
<li>QNTX utilization: 5%</li>
<li>Other process (PID 12345): 90%</li>
</ul>
<p>Then PID 12345 is violating fair share (should be ~70%).</p>
<h3>Response Strategies</h3>
<p><strong>1. Self-Protective Throttling:</strong></p>
<p>When GPU hog detected, throttle even more aggressively:</p>
<pre><code class="language-go">if hogs := sysMonitor.DetectGPUHogs(80.0, 10*time.Minute); len(hogs) &gt; 0 {
    log.Printf("WARNING: GPU hog: %s (PID %d) at %.1f%% for %.1f min",
        hogs[0].Name, hogs[0].PID, hogs[0].GPUUtilPercent, hogs[0].DurationMinutes)
    return baseQuota * 0.1  // Ultra-conservative when hog present
}
</code></pre>
<p><strong>2. Alerting and Logging:</strong></p>
<ul>
<li>Log GPU hogs: "Detected GPU hog: python (PID 12345) at 95% for 30 minutes"</li>
<li>Optional alerts (email, Slack, PagerDuty)</li>
<li>Track historical patterns: "python training job monopolizes GPU every night 8pm-6am"</li>
</ul>
<p><strong>3. Admin Reporting:</strong></p>
<p>Generate reports for sysadmins:</p>
<pre><code class="language-bash">qntx system gpu-report --last 24h
</code></pre>
<p>Output:</p>
<pre><code>GPU Utilization Report (Last 24 hours)

Fair Share Violations:
- python (PID 12345): 18.5 hours at &gt;90% (expected quota: 30%)
- inference-server (PID 67890): 12.2 hours at &gt;70% (expected quota: 40%)

Recommendation: Contact owners or adjust quotas
</code></pre>
<p><strong>4. Cooperative vs Defensive Mode:</strong></p>
<p>Auto-detect environment behavior:</p>
<pre><code class="language-go">if historicalHogFrequency &gt; 0.3 {  // Hogs detected &gt;30% of time
    switchToDefensiveMode()
}
</code></pre>
<ul>
<li><strong>Cooperative mode</strong>: Assume other processes will yield</li>
<li><strong>Defensive mode</strong>: Assume other processes won't yield, preserve resources conservatively</li>
</ul>
<p><strong>5. Advanced: cgroups Enforcement (Linux only):</strong></p>
<p>If running with sufficient privileges:</p>
<ul>
<li>Create GPU cgroup with 30% utilization cap</li>
<li>Move QNTX process into cgroup</li>
<li>Kernel enforces limit even if other processes misbehave</li>
</ul>
<p>Note: Requires root or CAP_SYS_ADMIN.</p>
<p><strong>6. Fallback: Time-Based Quota:</strong></p>
<p>If GPU constantly saturated:</p>
<ul>
<li>"You can use GPU between 9am-11am daily" (negotiated with admin)</li>
<li>QNTX runs jobs only during allocated time windows</li>
<li>Ignore real-time utilization, trust time-based quota</li>
</ul>
<h2>Configuration</h2>
<pre><code class="language-toml">[pulse.gpu]
hog_detection_enabled = true
hog_threshold_percent = 80.0     # Processes using &gt;80% considered hogs
hog_duration_minutes = 10.0      # Must sustain for 10+ minutes
defensive_quota_multiplier = 0.1 # Use 10% of quota when hog detected
alert_on_hog = true              # Send alerts when hogs detected
</code></pre>
<h2>Migration Path</h2>
<ol>
<li>Add SystemResourceMonitor to pulse package (initially returns dummy values)</li>
<li>Implement nvidia-smi parsing for GPU (Linux only, graceful fallback on Mac/Windows)</li>
<li>Add adaptive quota logic to Tracker.CheckBudget()</li>
<li>Add system load check to worker.processJobs() with exponential backoff</li>
<li>Add configuration flags: max_system_gpu_percent, backpressure_threshold</li>
</ol>
<h2>Benefits</h2>
<p>QNTX becomes a "good citizen" on shared infrastructure:</p>
<ul>
<li>Won't starve other processes when they need GPU</li>
<li>Automatically throttles during peak load times</li>
<li>Respects system-wide resource allocation policies</li>
<li>Enables multi-tenant deployment (multiple users on same GPU server)</li>
</ul>
<h2>Real-World Scenario</h2>
<p>You're allocated 30% GPU capacity on shared server. Another user's training job runs 24/7 at 95% GPU, violating fair share. QNTX detects this:</p>
<ol>
<li>Logs: "GPU hog detected: python (PID 12345) at 95% for 8 hours"</li>
<li>Switches to defensive mode: reduces own quota from 30% to 3%</li>
<li>Preserves resources for critical user-initiated operations (--sync flag)</li>
<li>Generates report for sysadmin: "User X's training job violating fair share"</li>
<li>Admin contacts User X to fix or adjusts cgroup limits</li>
</ol>
<p><strong>Result:</strong> QNTX protects itself from badly behaved neighbors while maintaining observability.</p>
<footer class="site-footer">
  <p class="provenance">Generated by sitegen.nix at commit <a href="https://github.com/teranos/QNTX/commit/b664e41fac56a47a886ef727bc4342996b929f1d-dirty">b664e41-dirty</a></p>
</footer>
</body>
</html>
